{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_DIR = tf.keras.utils.get_file(\n",
    "    \"modelnet.zip\",\n",
    "    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"ModelNet10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# bathtub 0, bed 1, chair 2, desk 3, dresser 4, monitor 5, night_stand 6, sofa 7, table 8, toliet 9\n",
    "item_names = [\"bathtub\", \"bed\", \"chair\", \"desk\", \"dresser\", \"monitor\", \"night_stand\", \"sofa\", \"table\", \"toliet\"]\n",
    "train_labels = np.array([])\n",
    "train_inputs = []\n",
    "test_labels = np.array([])\n",
    "test_inputs = []\n",
    "\n",
    "\n",
    "for i in range(len(item_names)):\n",
    "# for i in range(2):\n",
    "    item_name = item_names[i]\n",
    "    \n",
    "    train_files = glob.glob(os.path.join(DATA_DIR, item_name, \"train/*.off\"))\n",
    "    train_labels = np.concatenate([train_labels, np.ones(len(train_files)) * i])\n",
    "    for file in train_files:\n",
    "        train_inputs.append(trimesh.load(file).sample(2048))\n",
    "        \n",
    "    test_files = glob.glob(os.path.join(DATA_DIR, item_name, \"test/*.off\"))\n",
    "    test_labels = np.concatenate([test_labels, np.ones(len(test_files)) * i])\n",
    "    for file in test_files:\n",
    "        test_inputs.append(trimesh.load(file).sample(2048))\n",
    "\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels).flatten()\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels).flatten()\n",
    "\n",
    "# normalization\n",
    "train_inputs = train_inputs - train_inputs.mean(dim=1).unsqueeze(1)\n",
    "train_inputs = train_inputs / torch.max(torch.sqrt(torch.sum(train_inputs ** 2, dim=2)), dim=1).values.view(-1, 1, 1)\n",
    "test_inputs = test_inputs - test_inputs.mean(dim=1).unsqueeze(1)\n",
    "test_inputs = test_inputs / torch.max(torch.sqrt(torch.sum(test_inputs ** 2, dim=2)), dim=1).values.view(-1, 1, 1)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "random_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import sys\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EPS = 1e-8\n",
    "WARMUP = 100\n",
    "num_classes = 10\n",
    "\n",
    "def update_progress(progress):\n",
    "    sys.stdout.write('\\r%d%%' % progress)\n",
    "    # sys.stdout.write(f'{progress}%  {msg}')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def format_time(time):\n",
    "    time_rounded = int(round((time)))\n",
    "    return str(datetime.timedelta(seconds=time_rounded))\n",
    "\n",
    "\n",
    "def train_model(model, epochs, datalodaer):\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    num_training_steps = len(datalodaer) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP, num_training_steps=num_training_steps)\n",
    "    \n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    batch_size = datalodaer.batch_size\n",
    "    num_data = len(datalodaer) * batch_size\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\" --- training model\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for step, batch in enumerate(datalodaer):\n",
    "            batch_inputs = tuple(t.to('cuda') for t in batch)\n",
    "            inputs = batch_inputs[0].type(torch.float32)\n",
    "            labels = batch_inputs[1]\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = F.cross_entropy(output, labels.long())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            update_progress((step+1)*batch_size / num_data * 100)\n",
    "\n",
    "        avg_train_loss = total_loss / len(datalodaer)\n",
    "        \n",
    "        \n",
    "        print(f' {epoch+1}/{epochs} - elapsed: {format_time(time.time() - epoch_start_time)}, average train loss: {avg_train_loss}')\n",
    "\n",
    "    print(f' --- train finished, elapsed: {format_time(time.time() - start_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import PointTransformer\n",
    "\n",
    "class PointTransformerCls(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = PointTransformer(3)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x) # b, n, 512\n",
    "        x = torch.mean(x, dim=1)\n",
    "\n",
    "        return self.cls(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointTransformerCls(\n",
       "  (transformer): PointTransformer(\n",
       "    (mlp): SingleMLP(\n",
       "      (fc): Linear(in_features=3, out_features=32, bias=True)\n",
       "      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): PointTransformerBlock(\n",
       "        (transformerLayer): PointTransformerLayer(\n",
       "          (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_delta1): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (fc_delta2): Linear(in_features=3, out_features=32, bias=True)\n",
       "          (bn_delta): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (fc_gamma1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_gamma2): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (bn_gamma1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn_gamma2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): TransitionDown(\n",
       "        (fc): Linear(in_features=35, out_features=64, bias=True)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): PointTransformerBlock(\n",
       "        (transformerLayer): PointTransformerLayer(\n",
       "          (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (fc_delta1): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (fc_delta2): Linear(in_features=3, out_features=64, bias=True)\n",
       "          (bn_delta): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (fc_gamma1): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (fc_gamma2): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (bn_gamma1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn_gamma2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): TransitionDown(\n",
       "        (fc): Linear(in_features=67, out_features=128, bias=True)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): PointTransformerBlock(\n",
       "        (transformerLayer): PointTransformerLayer(\n",
       "          (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_delta1): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (fc_delta2): Linear(in_features=3, out_features=128, bias=True)\n",
       "          (bn_delta): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (fc_gamma1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (fc_gamma2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (bn_gamma1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn_gamma2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): TransitionDown(\n",
       "        (fc): Linear(in_features=131, out_features=256, bias=True)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): PointTransformerBlock(\n",
       "        (transformerLayer): PointTransformerLayer(\n",
       "          (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_delta1): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (fc_delta2): Linear(in_features=3, out_features=256, bias=True)\n",
       "          (bn_delta): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (fc_gamma1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_gamma2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (bn_gamma1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn_gamma2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): TransitionDown(\n",
       "        (fc): Linear(in_features=259, out_features=512, bias=True)\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (8): PointTransformerBlock(\n",
       "        (transformerLayer): PointTransformerLayer(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_delta1): Linear(in_features=3, out_features=3, bias=True)\n",
       "          (fc_delta2): Linear(in_features=3, out_features=512, bias=True)\n",
       "          (bn_delta): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (fc_gamma1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_gamma2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (bn_gamma1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (bn_gamma2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = PointTransformerCls(10)\n",
    "# model.train()\n",
    "# train_model(model, 10, train_dataloader)\n",
    "# torch.save(model, \"./p_transformer.pt\")\n",
    "model = torch.load(\"p_transformer.pt\")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, draw = False):\n",
    "    test_loss = 0\n",
    "    labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "\n",
    "    model.to('cuda')\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = dataloader.batch_size\n",
    "    num_data = len(dataloader) * batch_size\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch_inputs = tuple(t.to('cuda') for t in batch)\n",
    "            inputs = batch_inputs[0].type(torch.float32)\n",
    "            label = batch_inputs[1]\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = F.cross_entropy(output, label.long())\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            softmaxed_output = F.softmax(output, dim=1)\n",
    "            prediction = softmaxed_output.argmax(dim=1).detach().cpu().numpy()\n",
    "            predictions = np.concatenate([predictions, prediction])\n",
    "            labels = np.concatenate([labels, label.cpu().long().numpy()])\n",
    "            \n",
    "\n",
    "            update_progress((step * batch_size) / num_data * 100)\n",
    "            \n",
    "    n_rights = predictions[predictions == labels].shape[0]\n",
    "    \n",
    "\n",
    "    test_loss /= num_data\n",
    "    print(f'\\nloss: {test_loss}, {n_rights}/{num_data}, f1_score : {f1_score(labels, predictions, average=\"micro\")}')\n",
    "    print(f' --- evaluation finished {format_time(time.time() - start_time)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n",
      "loss: 0.032046578397723964, 741/808, f1_score : 0.9170792079207921\n",
      " --- evaluation finished 0:00:25\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
