{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 02:24:39.765696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
      "473402300/473402300 [==============================] - 60s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_DIR = tf.keras.utils.get_file(\n",
    "    \"modelnet.zip\",\n",
    "    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"ModelNet10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4294/1552365447.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343964576/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  train_inputs = torch.tensor(train_inputs)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 8\n",
    "\n",
    "# bathtub 0, bed 1, chair 2, desk 3, dresser 4, monitor 5, night_stand 6, sofa 7, table 8, toilet 9\n",
    "item_names = [\"bathtub\", \"bed\", \"chair\", \"desk\", \"dresser\", \"monitor\", \"night_stand\", \"sofa\", \"table\", \"toilet\"]\n",
    "train_labels = np.array([])\n",
    "train_inputs = []\n",
    "test_labels = np.array([])\n",
    "test_inputs = []\n",
    "\n",
    "\n",
    "for i in range(len(item_names)):\n",
    "# for i in range(2):\n",
    "    item_name = item_names[i]\n",
    "    \n",
    "    train_files = glob.glob(os.path.join(DATA_DIR, item_name, \"train/*.off\"))\n",
    "    train_labels = np.concatenate([train_labels, np.ones(len(train_files)) * i])\n",
    "    for file in train_files:\n",
    "        train_inputs.append(trimesh.load(file).sample(4096))\n",
    "        \n",
    "    test_files = glob.glob(os.path.join(DATA_DIR, item_name, \"test/*.off\"))\n",
    "    test_labels = np.concatenate([test_labels, np.ones(len(test_files)) * i])\n",
    "    for file in test_files:\n",
    "        test_inputs.append(trimesh.load(file).sample(4096))\n",
    "\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels).flatten()\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels).flatten()\n",
    "\n",
    "# normalization\n",
    "train_inputs = train_inputs - train_inputs.mean(dim=1).unsqueeze(1)\n",
    "train_inputs = train_inputs / torch.max(torch.sqrt(torch.sum(train_inputs ** 2, dim=2)), dim=1).values.view(-1, 1, 1)\n",
    "test_inputs = test_inputs - test_inputs.mean(dim=1).unsqueeze(1)\n",
    "test_inputs = test_inputs / torch.max(torch.sqrt(torch.sum(test_inputs ** 2, dim=2)), dim=1).values.view(-1, 1, 1)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "random_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'train_inputs': train_inputs,\n",
    "    'train_labels': train_labels,\n",
    "    'test_inputs': test_inputs,\n",
    "    'test_labels': test_labels\n",
    "}, 'cls_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import sys\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "EPS = 1e-8\n",
    "WARMUP = 100\n",
    "\n",
    "def update_progress(progress):\n",
    "    sys.stdout.write('\\r%d%%' % progress)\n",
    "    # sys.stdout.write(f'{progress}%  {msg}')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def format_time(time):\n",
    "    time_rounded = int(round((time)))\n",
    "    return str(datetime.timedelta(seconds=time_rounded))\n",
    "\n",
    "\n",
    "def train_model(model, epochs, datalodaer):\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    num_training_steps = len(datalodaer) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP, num_training_steps=num_training_steps)\n",
    "    \n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "\n",
    "    batch_size = datalodaer.batch_size\n",
    "    num_data = len(datalodaer) * batch_size\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\" --- training model\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for step, batch in enumerate(datalodaer):\n",
    "            batch_inputs = tuple(t.to('cuda') for t in batch)\n",
    "            inputs = batch_inputs[0].type(torch.float32)\n",
    "            labels = batch_inputs[1]\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = F.cross_entropy(output, labels.long())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            update_progress((step+1)*batch_size / num_data * 100)\n",
    "\n",
    "        avg_train_loss = total_loss / len(datalodaer)\n",
    "        \n",
    "        \n",
    "        print(f' {epoch+1}/{epochs} - elapsed: {format_time(time.time() - epoch_start_time)}, average train loss: {avg_train_loss}')\n",
    "\n",
    "    print(f' --- train finished, elapsed: {format_time(time.time() - start_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model import PointTransformer\n",
    "\n",
    "class PointTransformerCls(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.transformer = PointTransformer(3)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x) # b, n, 512\n",
    "        x = torch.mean(x, dim=1)\n",
    "\n",
    "        return self.cls(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeti/anaconda3/envs/py309/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- training model\n",
      "100% 1/10 - elapsed: 0:05:21, average train loss: 0.8805732654859284\n",
      "100% 2/10 - elapsed: 0:05:22, average train loss: 0.4353672843267824\n",
      "100% 3/10 - elapsed: 0:05:23, average train loss: 0.3224945923958332\n",
      "100% 4/10 - elapsed: 0:05:54, average train loss: 0.24662769633370502\n",
      "100% 5/10 - elapsed: 0:06:14, average train loss: 0.2052340107808498\n",
      "100% 6/10 - elapsed: 0:06:14, average train loss: 0.17295985943354333\n",
      "100% 7/10 - elapsed: 0:06:13, average train loss: 0.1463711571493782\n",
      "100% 8/10 - elapsed: 0:06:14, average train loss: 0.11660846313025978\n",
      "100% 9/10 - elapsed: 0:06:11, average train loss: 0.09290352984688084\n",
      "100% 10/10 - elapsed: 0:05:58, average train loss: 0.08250303392468593\n",
      " --- train finished, elapsed: 0:59:04\n"
     ]
    }
   ],
   "source": [
    "model = PointTransformerCls(num_classes)\n",
    "model.cuda()\n",
    "model.train()\n",
    "train_model(model, 10, train_dataloader)\n",
    "torch.save(model, \"./cls_trained.pt\")\n",
    "# model = torch.load(\"cls_trained.pt\")\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, draw = False):\n",
    "    test_loss = 0\n",
    "    labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "\n",
    "    model.to('cuda')\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = dataloader.batch_size\n",
    "    num_data = len(dataloader) * batch_size\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch_inputs = tuple(t.to('cuda') for t in batch)\n",
    "            inputs = batch_inputs[0].type(torch.float32)\n",
    "            label = batch_inputs[1]\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = F.cross_entropy(output, label.long())\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            softmaxed_output = F.softmax(output, dim=1)\n",
    "            prediction = softmaxed_output.argmax(dim=1).detach().cpu().numpy()\n",
    "            predictions = np.concatenate([predictions, prediction])\n",
    "            labels = np.concatenate([labels, label.cpu().long().numpy()])\n",
    "            \n",
    "\n",
    "            update_progress((step * batch_size) / num_data * 100)\n",
    "            \n",
    "    n_rights = predictions[predictions == labels].shape[0]\n",
    "    \n",
    "\n",
    "    test_loss /= num_data\n",
    "    print(f'\\nloss: {test_loss}, {n_rights}/{num_data}, f1_score : {f1_score(labels, predictions, average=\"micro\")}')\n",
    "    print(f' --- evaluation finished {format_time(time.time() - start_time)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n",
      "loss: 0.025643586738504018, 839/912, f1_score : 0.9240088105726872\n",
      " --- evaluation finished 0:01:05\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
